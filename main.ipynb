{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After read SyncTweedies (https://arxiv.org/pdf/2403.14370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%cd /content\n",
    "\n",
    "if os.path.exists('/content/hiwyn'):\n",
    "  %rm -rf /content/hiwyn\n",
    "!git clone https://github.com/ByeongHyunPak/hiwyn.git\n",
    "\n",
    "%cd /content/hiwyn\n",
    "\n",
    "# !sudo apt-get install ninja-build\n",
    "\n",
    "# if os.path.exists('/content/hiwyn/nvdiffrast'):\n",
    "#   %rm -rf /content/hiwyn/nvdiffrast\n",
    "# !git clone --recursive https://github.com/NVlabs/nvdiffrast\n",
    "\n",
    "# %cd /content/hiwyn/nvdiffrast\n",
    "\n",
    "# !pip install .\n",
    "\n",
    "%cd /content/hiwyn\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# import nvdiffrast.torch as dr\n",
    "# use_opengl = False # On T4 GPU, only False works, but rasterizer works much better if = True\n",
    "# glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry import make_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from geometry import gridy2x_pers2erp, gridy2x_erp2pers\n",
    "\n",
    "class HIWYN_MD_ERP2PERS_0_0_0(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        \n",
    "        self.views = [\n",
    "            (0.0, 0.0), (-45.0, 0.0)\n",
    "        ]\n",
    "\n",
    "        self.fin_v_nums = []\n",
    "        self.indices = None\n",
    "    \n",
    "    def decode_and_save(self, i, latents, save_dir, buffer, tag='pers'):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, latent in enumerate(latents):\n",
    "            pers_img = self.decode_latents(latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        buffer.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/{tag}_{theta}_{phi}.png')\n",
    "        \n",
    "        return buffer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cond_noise_sampling(self, src_noise):\n",
    "\n",
    "        B, C, H, W = src_noise.shape\n",
    "        up_factor = 2 ** self.up_level\n",
    "        upscaled_means = F.interpolate(src_noise, scale_factor=(up_factor, up_factor), mode='nearest')\n",
    "\n",
    "        up_H = up_factor * H\n",
    "        up_W = up_factor * W\n",
    "\n",
    "        # 1) Unconditionally sample a discrete Nk x Nk Gaussian sample\n",
    "        raw_rand = torch.randn(B, C, up_H, up_W, device=src_noise.device)\n",
    "\n",
    "        # 2) Remove its mean from it\n",
    "        Z_mean = raw_rand.unfold(2, up_factor, up_factor).unfold(3, up_factor, up_factor).mean((4, 5))\n",
    "        Z_mean = F.interpolate(Z_mean, scale_factor=up_factor, mode='nearest')\n",
    "        mean_removed_rand = raw_rand - Z_mean\n",
    "\n",
    "        # 3) Add the pixel value to it\n",
    "        up_noise = upscaled_means / up_factor + mean_removed_rand\n",
    "\n",
    "        return up_noise # sqrt(N_k) W(A_k): sub-pixel noise scaled with sqrt(N_k) ~ N(0, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def discrete_warping(self, src_white_noise, hw=(512//8, 512//8), normalize_fin_v_val=True):\n",
    "        B, C, H, W = src_white_noise.shape\n",
    "        h, w = hw\n",
    "\n",
    "        erp_grid = make_coord((H, W), flatten=False).to(self.device) # (H, W, 2)\n",
    "        erp_up_noise_flat = src_white_noise.reshape(B*C, -1)\n",
    "\n",
    "        pers_idx = torch.arange(1, h*w+1, device=self.device, dtype=torch.float32).view(1, h, w) # (1, h, w)\n",
    "        ones_flat = torch.ones_like(erp_up_noise_flat[:1])\n",
    "\n",
    "        tgts = []\n",
    "        indices = []\n",
    "\n",
    "        for theta, phi in self.views:\n",
    "            erp2pers_grid, valid_mask = gridy2x_erp2pers(gridy=erp_grid,\n",
    "                HWy=(H, W), HWx=hw, THETA=theta, PHI=phi, FOVy=360, FOVx=90)\n",
    "            erp2pers_grid = erp2pers_grid.view(H, W, 2)\n",
    "            valid_mask = valid_mask.view(1, H, W)\n",
    "            \n",
    "            # Find nearest grid index of erp pixel on Pers. grid\n",
    "            erp2pers_idx = F.grid_sample(\n",
    "                pers_idx.unsqueeze(0),\n",
    "                erp2pers_grid.unsqueeze(0).flip(-1),\n",
    "                mode=\"nearest\", align_corners=False)[0] # (1, H, W)\n",
    "            erp2pers_idx *= valid_mask # non-mapped pixel has 0 value.\n",
    "            erp2pers_idx = erp2pers_idx.to(torch.int64)\n",
    "\n",
    "            ind_flat = erp2pers_idx.view(1, -1)\n",
    "\n",
    "            # 5) Get warped target noise\n",
    "            fin_v_val = torch.zeros(B*C, h*w+1, device=self.device) \\\n",
    "                .scatter_add_(1, index=ind_flat.repeat(B*C, 1), src=erp_up_noise_flat)[..., 1:]\n",
    "            fin_v_num = torch.zeros(1, h*w+1, device=self.device) \\\n",
    "                .scatter_add_(1, index=ind_flat, src=ones_flat)[..., 1:]\n",
    "            assert fin_v_num.min() != 0, ValueError(f\"{theta},{phi}\")\n",
    "\n",
    "            if normalize_fin_v_val:\n",
    "                final_values = fin_v_val / torch.sqrt(fin_v_num)\n",
    "            else:\n",
    "                final_values = fin_v_val\n",
    "            tgt_warped_noise = final_values.reshape(B, C, h, w).float()\n",
    "            tgt_warped_noise = tgt_warped_noise.to(self.device)\n",
    "            \n",
    "            tgts.append(tgt_warped_noise)\n",
    "            indices.append(erp2pers_idx.reshape((H, W)))\n",
    "\n",
    "            if len(self.fin_v_nums) < len(self.views):\n",
    "                self.fin_v_nums.append(fin_v_num.reshape(h, w))\n",
    "        \n",
    "        self.indices = indices\n",
    "        return tgts\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # 1) Initialize x^T_ERP\n",
    "        x_erp = torch.randn((1, 4, height//8, width//8), device=self.device)\n",
    "\n",
    "        # 2) Conditional Upsampling x^T_ERP\n",
    "        x_erp_up = self.cond_noise_sampling(x_erp)\n",
    "\n",
    "        # 3) Discrete warping x^T_ERP to {w^T_i}_i=1:N\n",
    "        w_js = self.discrete_warping(x_erp_up)\n",
    "        buffer_imgs = self.decode_and_save(-1, w_js, save_dir, [])\n",
    "\n",
    "        # set scheduler\n",
    "        value = torch.zeros_like(x_erp_up)\n",
    "        count = torch.zeros_like(x_erp_up)\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "\n",
    "            value.zero_()\n",
    "            count.zero_()\n",
    "\n",
    "            for j, w_j in enumerate(w_js):\n",
    "\n",
    "                # 4) Predict e_theta(w^t_j, t)\n",
    "                unet_input = torch.cat([w_j] * 2)\n",
    "                noise_pred = self.unet(unet_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # 5) Get w'^t-1_j\n",
    "                w_j_denoised = self.scheduler.step(noise_pred, t, w_j)['prev_sample']\n",
    "\n",
    "                # 6) inverse mapping w -> x\n",
    "                x_erp_up_j, mask_x_j = self.inverse_mapping(w_j_denoised, j)\n",
    "\n",
    "                value[:, :] += x_erp_up_j * mask_x_j\n",
    "                count[:, :] += mask_x_j\n",
    "            \n",
    "            # save count map\n",
    "            count_ = count / count.max()\n",
    "            count_ = ToPILImage()(count_.cpu()[0][0])\n",
    "            count_.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "            # 7) Aggregate on canonical space\n",
    "            x_erp_up = value / (count + 1e-8)\n",
    "\n",
    "            # 8) forward mapping x -> w\n",
    "            w_js = self.forward_mapping(x_erp_up)\n",
    "            buffer_imgs = self.decode_and_save(i, w_js, save_dir, buffer_imgs)\n",
    "        \n",
    "        return buffer_imgs\n",
    "\n",
    "    def forward_mapping(self, x):\n",
    "        tgts = self.discrete_warping(x, normalize_fin_v_val=False)\n",
    "        return tgts\n",
    "\n",
    "    def inverse_mapping(self, w_j, j):\n",
    "        \n",
    "        B, C, h, w = w_j.shape\n",
    "        H, W  = self.indices[j].shape\n",
    "\n",
    "        sub_x_j = torch.zeros(B*C, H*W, device=self.device)\n",
    "        w_j_flat = w_j.reshape(B*C, -1) / self.fin_v_nums[j].view(1, -1)\n",
    "        w_j_flat_pad = torch.zeros(B*C, h*w + 1, device=self.device)\n",
    "        w_j_flat_pad[:, 1:] = w_j_flat\n",
    "\n",
    "        ind_x2w_j = self.indices[j]\n",
    "        mask_x_j = ind_x2w_j > 0\n",
    "        \n",
    "        sub_x_j[:, ...] = w_j_flat_pad[:, ind_x2w_j.flatten()]\n",
    "        sub_x_j = sub_x_j.reshape(B, C, H, W)\n",
    "        \n",
    "        return sub_x_j, mask_x_j\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from geometry import gridy2x_pers2erp\n",
    "\n",
    "class HIWYN_MD_ERP2PERS_0_1_0(HIWYN_MD_ERP2PERS_0_0_0):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        \n",
    "        self.views = [\n",
    "            (0.0, 0.0), (-45.0, 0.0)\n",
    "        ]\n",
    "\n",
    "        self.fin_v_nums = []\n",
    "        self.indices = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # 1) Initialize x^T_ERP\n",
    "        x_erp = torch.randn((1, 4, height//8, width//8), device=self.device)\n",
    "\n",
    "        # 2) Conditional Upsampling x^T_ERP\n",
    "        x_erp_up = self.cond_noise_sampling(x_erp)\n",
    "\n",
    "        # 3) Discrete warping x^T_ERP to {w^T_i}_i=1:N\n",
    "        w_js = self.discrete_warping(x_erp_up)\n",
    "        buffer_imgs = self.decode_and_save(-1, w_js, save_dir, [])\n",
    "\n",
    "        # set scheduler\n",
    "        value = torch.zeros_like(x_erp_up)\n",
    "        count = torch.zeros_like(x_erp_up)\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "\n",
    "            value.zero_()\n",
    "            count.zero_()\n",
    "\n",
    "            for j, w_j in enumerate(w_js):\n",
    "\n",
    "                # 4) Predict e_theta(w^t_j, t)\n",
    "                unet_input = torch.cat([w_j] * 2)\n",
    "                noise_pred = self.unet(unet_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # 5) Get w'^t-1_j, w'^0_j\n",
    "                ddim_output = self.scheduler.step(noise_pred, t, w_j)\n",
    "                w_j_denoised = ddim_output['prev_sample']\n",
    "                w_j_original = ddim_output['pred_original_sample']\n",
    "                img_j = self.decode_latents(w_j_original)               \n",
    "                theta, phi = self.views[j]\n",
    "                ToPILImage()(img_j[0].cpu()).save(f'/{save_dir}/{i+1:0>2}/w^0_{theta}_{phi}.png')\n",
    "                \n",
    "                # 6) inverse mapping w'^0 -> x\n",
    "                x_erp_up_j, mask_x_j = self.inverse_mapping(w_j_original, j)\n",
    "\n",
    "                value[:, :] += x_erp_up_j * mask_x_j\n",
    "                count[:, :] += mask_x_j\n",
    "            \n",
    "            # save count map\n",
    "            count_ = count / count.max()\n",
    "            count_ = ToPILImage()(count_.cpu()[0][0])\n",
    "            count_.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "            # 7) Aggregate on canonical space\n",
    "            x_erp_up = value / (count + 1e-8)\n",
    "            \n",
    "            ### 0.1.0 - SyncTweedies\n",
    "\n",
    "            # 8) forward mapping x -> w^0\n",
    "            w0_js = self.forward_mapping(x_erp_up)\n",
    "            \n",
    "            # 9) ddim scheduler step w/ w^t & w^0\n",
    "            w_new_js = []\n",
    "            for w_j, w0_j in zip(w_js, w0_js):\n",
    "                timestep = t\n",
    "                prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "                alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "\n",
    "                beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "                variance = self.scheduler._get_variance(timestep, prev_timestep)\n",
    "                std_dev_t = 0.0\n",
    "\n",
    "                xt_coeff = torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2) / torch.sqrt(1 - alpha_prod_t)\n",
    "                x0_coeff = torch.sqrt(alpha_prod_t_prev) - torch.sqrt(alpha_prod_t) / torch.sqrt(1 - alpha_prod_t) * torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2)\n",
    "                \n",
    "                w_new_j = xt_coeff * w_j + x0_coeff * w0_j\n",
    "                w_new_js.append(w_new_j)\n",
    "            w_js = w_new_js\n",
    "            \n",
    "            buffer_imgs = self.decode_and_save(i, w_js, save_dir, buffer_imgs)\n",
    "        \n",
    "        return buffer_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from geometry import gridy2x_pers2erp\n",
    "\n",
    "class HIWYN_MD_ERP2PERS_0_1_1(HIWYN_MD_ERP2PERS_0_1_0):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        \n",
    "        self.views = [\n",
    "            (0.0, 0.0), (-45.0, 0.0)\n",
    "        ]\n",
    "\n",
    "        self.fin_v_nums = []\n",
    "        self.indices = None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, imgs):\n",
    "        imgs = (imgs - 0.5) * 2\n",
    "        posterior = self.vae.encode(imgs).latent_dist\n",
    "        latents = posterior.sample() * 0.18215\n",
    "        return latents\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def img_j_to_erp(self, img_j, j, erp_HW=(1024, 2048)):\n",
    "        H, W = erp_HW\n",
    "\n",
    "        theta, phi = self.views[j]\n",
    "\n",
    "        erp_grid = make_coord(erp_HW, flatten=False).to(self.device)\n",
    "\n",
    "        erp2pers_grid, valid_mask = gridy2x_erp2pers(gridy=erp_grid,\n",
    "            HWy=erp_HW, HWx=img_j.shape[-2:], THETA=theta, PHI=phi, FOVy=360, FOVx=90)\n",
    "        \n",
    "        erp2pers_grid = erp2pers_grid.view(H, W, 2)\n",
    "        valid_mask = valid_mask.view(1, 1, H, W)\n",
    "\n",
    "        img_j_to_erp_img = F.grid_sample(\n",
    "            img_j,\n",
    "            erp2pers_grid.unsqueeze(0).flip(-1),\n",
    "            mode=\"bicubic\", align_corners=False)\n",
    "        img_j_to_erp_img *= valid_mask\n",
    "\n",
    "        img_j_to_erp_img.clamp_(0, 1)\n",
    "\n",
    "        return img_j_to_erp_img, valid_mask\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def erp_to_img_j(self, erp_img, img_j_hw=(512, 512)):\n",
    "        B, C, H, W = erp_img.shape\n",
    "        h, w = img_j_hw\n",
    "\n",
    "        pers_grid = make_coord(img_j_hw, flatten=False).to(self.device)\n",
    "        img_js = []\n",
    "        for theta, phi in self.views:\n",
    "            pers2erp_grid, valid_mask = gridy2x_pers2erp(gridy=pers_grid,\n",
    "                HWy=img_j_hw, HWx=(H, W), THETA=theta, PHI=phi, FOVy=90, FOVx=360)\n",
    "            \n",
    "            pers2erp_grid = pers2erp_grid.view(h, w, 2)\n",
    "            valid_mask = valid_mask.view(1, 1, h, w)\n",
    "\n",
    "            img_j = F.grid_sample(\n",
    "                erp_img,\n",
    "                pers2erp_grid.unsqueeze(0).flip(-1),\n",
    "                mode=\"bicubic\", align_corners=False)\n",
    "            img_j *= valid_mask\n",
    "\n",
    "            img_j.clamp_(0, 1)\n",
    "            \n",
    "            img_js.append(img_j)\n",
    "        return img_js\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # 1) Initialize x^T_ERP\n",
    "        x_erp = torch.randn((1, 4, height//8, width//8), device=self.device)\n",
    "\n",
    "        # 2) Conditional Upsampling x^T_ERP\n",
    "        x_erp_up = self.cond_noise_sampling(x_erp)\n",
    "\n",
    "        # 3) Discrete warping x^T_ERP to {w^T_i}_i=1:N\n",
    "        w_js = self.discrete_warping(x_erp_up)\n",
    "        buffer_imgs = self.decode_and_save(-1, w_js, save_dir, [])\n",
    "\n",
    "        # set scheduler\n",
    "        # value = torch.zeros_like(x_erp_up)\n",
    "        # count = torch.zeros_like(x_erp_up)\n",
    "        value = torch.zeros((1, 3, *x_erp_up.shape[-2:]), device=self.device)\n",
    "        count = torch.zeros((1, 3, *x_erp_up.shape[-2:]), device=self.device)\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "\n",
    "            value.zero_()\n",
    "            count.zero_()\n",
    "\n",
    "            for j, w_j in enumerate(w_js):\n",
    "\n",
    "                # 4) Predict e_theta(w^t_j, t)\n",
    "                unet_input = torch.cat([w_j] * 2)\n",
    "                noise_pred = self.unet(unet_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # 5) Get w'^t-1_j, w'^0_j\n",
    "                ddim_output = self.scheduler.step(noise_pred, t, w_j)\n",
    "                w_j_denoised = ddim_output['prev_sample']\n",
    "                w_j_original = ddim_output['pred_original_sample']\n",
    "                \n",
    "                ### 0.1.1 - fusion on decoded latents\n",
    "                img_j = self.decode_latents(w_j_original)               \n",
    "                theta, phi = self.views[j]\n",
    "                ToPILImage()(img_j[0].cpu()).save(f'/{save_dir}/{i+1:0>2}/w^0_{theta}_{phi}.png')\n",
    "                # img_j = F.interpolate(img_j, scale_factor=1/8, mode='bilinear', align_corners=False)\n",
    "\n",
    "                # 6) inverse mapping w'^0 -> x\n",
    "                # x_erp_up_j, mask_x_j = self.inverse_mapping(img_j, j)\n",
    "                x_erp_up_j, mask_x_j = self.img_j_to_erp(img_j, j)\n",
    "\n",
    "                value[:, :] += x_erp_up_j * mask_x_j\n",
    "                count[:, :] += mask_x_j\n",
    "            \n",
    "            # save count map\n",
    "            count_ = count / count.max()\n",
    "            count_ = ToPILImage()(count_.cpu()[0][0])\n",
    "            count_.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "            # 7) Aggregate on canonical space\n",
    "            x_erp_up = value / (count + 1e-8)\n",
    "            ToPILImage()(x_erp_up[0].cpu()).save(f\"/{save_dir}/{i+1:0>2}/erp.png\")\n",
    "\n",
    "            # 8) forward mapping x -> w^0\n",
    "            ### 0.1.1 - re-encode the fused images\n",
    "            # img_js = self.forward_mapping(x_erp_up)\n",
    "            img_js = self.erp_to_img_j(x_erp_up)\n",
    "            w0_js = []\n",
    "            for img_j in img_js:\n",
    "                # img_j = F.interpolate(img_j, scale_factor=8, mode='bicubic', align_corners=False)\n",
    "                w0_j = self.encode_images(img_j)\n",
    "                w0_js.append(w0_j)            \n",
    "            \n",
    "            # 9) ddim scheduler step w/ w^t & w^0\n",
    "            w_new_js = []\n",
    "            for w_j, w0_j in zip(w_js, w0_js):\n",
    "                timestep = t\n",
    "                prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "                alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "\n",
    "                beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "                variance = self.scheduler._get_variance(timestep, prev_timestep)\n",
    "                std_dev_t = 0.0\n",
    "\n",
    "                xt_coeff = torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2) / torch.sqrt(1 - alpha_prod_t)\n",
    "                x0_coeff = torch.sqrt(alpha_prod_t_prev) - torch.sqrt(alpha_prod_t) / torch.sqrt(1 - alpha_prod_t) * torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2)\n",
    "                \n",
    "                w_new_j = xt_coeff * w_j + x0_coeff * w0_j\n",
    "                w_new_js.append(w_new_j)\n",
    "            w_js = w_new_js\n",
    "            \n",
    "            buffer_imgs = self.decode_and_save(i, w_js, save_dir, buffer_imgs)\n",
    "        \n",
    "        return buffer_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from geometry import gridy2x_pers2erp\n",
    "\n",
    "class HIWYN_MD_ERP2PERS_0_1_2(HIWYN_MD_ERP2PERS_0_1_1):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        \n",
    "        self.views = [\n",
    "            (0.0, 0.0), (90.0, 0.0), (180.0, 0.0), (270.0, 0.0) ## 0.1.2 - no overlap views\n",
    "        ]\n",
    "\n",
    "        self.fin_v_nums = []\n",
    "        self.indices = None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, imgs):\n",
    "        imgs = (imgs - 0.5) * 2\n",
    "        posterior = self.vae.encode(imgs).latent_dist\n",
    "        latents = posterior.sample() * 0.18215\n",
    "        return latents\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # 1) Initialize x^T_ERP\n",
    "        x_erp = torch.randn((1, 4, height//8, width//8), device=self.device)\n",
    "\n",
    "        # 2) Conditional Upsampling x^T_ERP\n",
    "        x_erp_up = self.cond_noise_sampling(x_erp)\n",
    "\n",
    "        # 3) Discrete warping x^T_ERP to {w^T_i}_i=1:N\n",
    "        w_js = self.discrete_warping(x_erp_up)\n",
    "        buffer_imgs = self.decode_and_save(-1, w_js, save_dir, [])\n",
    "\n",
    "        # set scheduler\n",
    "        # value = torch.zeros_like(x_erp_up)\n",
    "        # count = torch.zeros_like(x_erp_up)\n",
    "        value = torch.zeros((1, 3, *x_erp_up.shape[-2:]), device=self.device)\n",
    "        count = torch.zeros((1, 3, *x_erp_up.shape[-2:]), device=self.device)\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "\n",
    "            value.zero_()\n",
    "            count.zero_()\n",
    "\n",
    "            for j, w_j in enumerate(w_js):\n",
    "\n",
    "                # 4) Predict e_theta(w^t_j, t)\n",
    "                unet_input = torch.cat([w_j] * 2)\n",
    "                noise_pred = self.unet(unet_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # 5) Get w'^t-1_j, w'^0_j\n",
    "                ddim_output = self.scheduler.step(noise_pred, t, w_j)\n",
    "                w_j_denoised = ddim_output['prev_sample']\n",
    "                w_j_original = ddim_output['pred_original_sample']\n",
    "                \n",
    "                ### 0.1.1 - fusion on decoded latents\n",
    "                img_j = self.decode_latents(w_j_original)               \n",
    "                theta, phi = self.views[j]\n",
    "                ToPILImage()(img_j[0].cpu()).save(f'/{save_dir}/{i+1:0>2}/w^0_{theta}_{phi}.png')\n",
    "                # img_j = F.interpolate(img_j, scale_factor=1/8, mode='bilinear', align_corners=False)\n",
    "\n",
    "                # 6) inverse mapping w'^0 -> x\n",
    "                # x_erp_up_j, mask_x_j = self.inverse_mapping(img_j, j)\n",
    "                x_erp_up_j, mask_x_j = self.img_j_to_erp(img_j, j)\n",
    "\n",
    "                value[:, :] += x_erp_up_j * mask_x_j\n",
    "                count[:, :] += mask_x_j\n",
    "            \n",
    "            # save count map\n",
    "            count_ = count / count.max()\n",
    "            count_ = ToPILImage()(count_.cpu()[0][0])\n",
    "            count_.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "            # 7) Aggregate on canonical space\n",
    "            x_erp_up = value / (count + 1e-8)\n",
    "\n",
    "            ### 0.1.2 - fill the unvalid region of ERP with random color\n",
    "            x_erp_up = torch.where(count!=0, x_erp_up, torch.randn((1, 3, 1, 1), device=self.device))\n",
    "            ToPILImage()(x_erp_up[0].cpu()).save(f\"/{save_dir}/{i+1:0>2}/erp.png\")\n",
    "\n",
    "            # 8) forward mapping x -> w^0\n",
    "            ### 0.1.1 - re-encode the fused images\n",
    "            # img_js = self.forward_mapping(x_erp_up)\n",
    "            self.views = [((theta+10)%360, phi) for theta, phi in self.views]  \n",
    "            img_js = self.erp_to_img_j(x_erp_up)\n",
    "            w0_js = []\n",
    "            for img_j in img_js:\n",
    "                # img_j = F.interpolate(img_j, scale_factor=8, mode='bicubic', align_corners=False)\n",
    "                w0_j = self.encode_images(img_j)\n",
    "                w0_js.append(w0_j)            \n",
    "            \n",
    "            # 9) ddim scheduler step w/ w^t & w^0\n",
    "            w_new_js = []\n",
    "            for w_j, w0_j in zip(w_js, w0_js):\n",
    "                timestep = t\n",
    "                prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "                alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "\n",
    "                beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "                variance = self.scheduler._get_variance(timestep, prev_timestep)\n",
    "                std_dev_t = 0.0\n",
    "\n",
    "                xt_coeff = torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2) / torch.sqrt(1 - alpha_prod_t)\n",
    "                x0_coeff = torch.sqrt(alpha_prod_t_prev) - torch.sqrt(alpha_prod_t) / torch.sqrt(1 - alpha_prod_t) * torch.sqrt(1 - alpha_prod_t_prev - std_dev_t**2)\n",
    "                \n",
    "                w_new_j = xt_coeff * w_j + x0_coeff * w0_j\n",
    "                w_new_js.append(w_new_j)\n",
    "            w_js = w_new_js\n",
    "            \n",
    "            buffer_imgs = self.decode_and_save(i, w_js, save_dir, buffer_imgs)\n",
    "        \n",
    "        return buffer_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    prompt = \"Realistic cityscape of Florence\"\n",
    "\n",
    "    H, W = 1024, 2048\n",
    "    sd = HIWYN_MD_ERP2PERS_0_1_1(device=device, sd_version=sd_version)\n",
    "    version = \"0.1.1\"\n",
    "    dir_name = \"imgs\"\n",
    "\n",
    "    if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "        os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "    if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}-{version}/') is False:\n",
    "        os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}-{version}/')\n",
    "\n",
    "    dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}-{version}'\n",
    "    outputs = sd.text2erp(prompt, negative, height=H, width=W, num_inference_steps=steps, save_dir=dir)\n",
    "\n",
    "    # GPU 메모리 정리\n",
    "    import gc\n",
    "    del outputs\n",
    "    del sd\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

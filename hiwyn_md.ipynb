{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('/content/hiwyn'):\n",
    "  %rm -rf /content/hiwyn\n",
    "!git clone https://github.com/ByeongHyunPak/hiwyn.git\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ninja-build\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/content/hiwyn/nvdiffrast'):\n",
    "  %rm -rf /content/hiwyn/nvdiffrast\n",
    "!git clone --recursive https://github.com/NVlabs/nvdiffrast\n",
    "%cd /content/hiwyn/nvdiffrast\n",
    "!pip install .\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import nvdiffrast.torch as dr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_opengl = False # On T4 GPU, only False works, but rasterizer works much better if = True\n",
    "glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from utils import cond_noise_sampling, erp2pers_latent_warping, compute_erp_up_noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_4(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = torch.zeros_like(erp_up_latent)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "                \n",
    "                count.zero_()\n",
    "                value.zero_()\n",
    "\n",
    "                for pers_latent, erp2pers_ind in zip(pers_latents, erp2pers_indices):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # # compute the denoising step with the reference model\n",
    "                    # pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_noise_pred for valid region\n",
    "                    erp_up_noise_pred, erp_up_valid_region = compute_erp_up_noise_pred(pers_noise_pred, erp2pers_ind, fin_v_num)\n",
    "                    erp_up_noise_denoised = self.scheduler.step(erp_up_noise_pred, t, erp_up_latent)['prev_sample']\n",
    "\n",
    "                    value += torch.where(erp_up_valid_region, erp_up_noise_denoised, torch.zeros_like(erp_up_noise_denoised))\n",
    "                    count += torch.where(erp_up_valid_region, torch.ones_like(erp_up_noise_denoised), torch.zeros_like(erp_up_noise_denoised))\n",
    "                \n",
    "                # average erp_up_latent on overlap region\n",
    "                count = torch.clamp(count, min=1) \n",
    "                erp_up_latent = value / count\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "        \n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic cityscape of Florence.\"\n",
    "\n",
    "H, W = 1024, 2048\n",
    "sd = ERPMultiDiffusion_v3_3(device=device, sd_version=sd_version)\n",
    "dir_name = \"imgs\"\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}/')\n",
    "\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "outputs = sd.text2erp(prompt, negative, height=H, width=W, num_inference_steps=steps, save_dir=dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

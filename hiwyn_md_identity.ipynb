{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('/content/hiwyn'):\n",
    "  %rm -rf /content/hiwyn\n",
    "!git clone https://github.com/ByeongHyunPak/hiwyn.git\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ninja-build\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/content/hiwyn/nvdiffrast'):\n",
    "  %rm -rf /content/hiwyn/nvdiffrast\n",
    "!git clone --recursive https://github.com/NVlabs/nvdiffrast\n",
    "%cd /content/hiwyn/nvdiffrast\n",
    "!pip install .\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import nvdiffrast.torch as dr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_opengl = False # On T4 GPU, only False works, but rasterizer works much better if = True\n",
    "glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion, get_views\n",
    "from utils import cond_noise_sampling, identity_latent_warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIWYN_identity_MultiDiffusion(MultiDiffusion):\n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, latents, save_dir, ret_imgs):\n",
    "\n",
    "        imgs = []\n",
    "        for k, latent in enumerate(latents):\n",
    "            pers_img = self.decode_latents(latent)\n",
    "            imgs.append((k, pers_img)) # [k, img]\n",
    "        ret_imgs.append((i+1, imgs)) # [i+1, [k, img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for k, im in imgs:\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{k}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def text2img(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, \n",
    "                 width=512, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5, \n",
    "                 save_dir=None):\n",
    "\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "\n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define panorama grid and get views\n",
    "        latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "        up_latent = cond_noise_sampling(latent, self.up_level)\n",
    "        hiwyn_latent = identity_latent_warping(up_latent, latent.shape[-2:], glctx)[0][0]\n",
    "        \n",
    "        views = get_views(height, width)\n",
    "        count = torch.zeros_like(latent)\n",
    "        value = torch.zeros_like(latent)\n",
    "        \n",
    "        hiwyn_count = torch.zeros_like(up_latent)\n",
    "        hiwyn_value = torch.zeros_like(up_latent)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.autocast('cuda'):\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, [latent, hiwyn_latent], save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "\n",
    "                count.zero_()\n",
    "                value.zero_()\n",
    "                hiwyn_count.zero_()\n",
    "                hiwyn_value.zero_()\n",
    "\n",
    "                for h_start, h_end, w_start, w_end in views:\n",
    "                    # TODO we can support batches, and pass multiple views at once to the unet\n",
    "                    latent_view = latent[:, :, h_start:h_end, w_start:w_end]\n",
    "                    hiwyn_view = hiwyn_latent[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    latent_model_input = torch.cat([latent_view] * 2)\n",
    "                    hiwyn_latent_model_input = torch.cat([hiwyn_view] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "                    hiwyn_noise_pred = self.unet(hiwyn_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "                    \n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                    hiwyn_noise_pred_uncond, hiwyn_noise_pred_cond = hiwyn_noise_pred.chunk(2)\n",
    "                    hiwyn_noise_pred = hiwyn_noise_pred_uncond + guidance_scale * (hiwyn_noise_pred_cond - hiwyn_noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "                    value[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised\n",
    "                    count[:, :, h_start:h_end, w_start:w_end] += 1\n",
    "\n",
    "                    # denoising up_latent with noise_pred\n",
    "                    up_noise_pred = torch.zeros_like(up_latent)\n",
    "                    up_noise_pred[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end] = F.interpolate(hiwyn_noise_pred, size=(512, 512), mode='nearest') / 8\n",
    "                    up_latent_denoised = self.scheduler.step(up_noise_pred, t, up_latent)['prev_sample']\n",
    "\n",
    "                    hiwyn_value[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end] = up_latent_denoised[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end]\n",
    "                    hiwyn_count[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end] += 1\n",
    "                    \n",
    "                    print()\n",
    "                    print(f\"{i+1} / latent_view             : power {latent_view.pow(2).mean():>8.5f}\")\n",
    "                    print(f\"{i+1} / noise_pred              : power {noise_pred.pow(2).mean():>8.5f}\")\n",
    "                    print(f\"{i+1} / latents_view_denoised   : power {latents_view_denoised.pow(2).mean():>8.5f}\")\n",
    "                    print(f\"{i+1} / up_latent               : power {up_latent.pow(2).mean():>8.5f}\")\n",
    "                    print(f\"{i+1} / up_noise_pred           : power {up_noise_pred[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end].pow(2).mean():>8.5f}\")\n",
    "                    print(f\"{i+1} / up_latent_denoised      : power {up_latent_denoised[:, :, 8*h_start:8*h_end, 8*w_start:8*w_end].pow(2).mean():>8.5f}\")\n",
    "\n",
    "                # take the MultiDiffusion step\n",
    "                latent = torch.where(count > 0, value / count, value)\n",
    "                up_latent_denoised = torch.where(hiwyn_count > 0, hiwyn_value / hiwyn_count, hiwyn_value)\n",
    "                hiwyn_latent = identity_latent_warping(up_latent_denoised, latent.shape[-2:], glctx)[0][0]\n",
    "                up_latent = up_latent_denoised\n",
    "                \n",
    "                hiwyn_count = hiwyn_count.float() / hiwyn_count.max().float()\n",
    "                hiwyn_count_img = ToPILImage()(hiwyn_count.cpu()[0][0])\n",
    "                hiwyn_count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "                \n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, [hiwyn_latent, latent], save_dir, imgs)\n",
    "\n",
    "        # Img latents -> imgs\n",
    "        imgs = self.decode_latents(latent)  # [1, 3, 512, 512]\n",
    "        img = T.ToPILImage()(imgs[0].cpu())\n",
    "\n",
    "        return [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic cityscape of Florence.\"\n",
    "\n",
    "H, W = 512, 1024\n",
    "sd = HIWYN_identity_MultiDiffusion(device=device, sd_version=sd_version)\n",
    "dir_name = \"imgs\"\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}/')\n",
    "\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "outputs = sd.text2img(prompt, negative, height=H, width=W, num_inference_steps=steps, save_dir=dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

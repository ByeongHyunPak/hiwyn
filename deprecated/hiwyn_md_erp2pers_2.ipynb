{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('/content/hiwyn'):\n",
    "  %rm -rf /content/hiwyn\n",
    "!git clone https://github.com/ByeongHyunPak/hiwyn.git\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ninja-build\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/content/hiwyn/nvdiffrast'):\n",
    "  %rm -rf /content/hiwyn/nvdiffrast\n",
    "!git clone --recursive https://github.com/NVlabs/nvdiffrast\n",
    "%cd /content/hiwyn/nvdiffrast\n",
    "!pip install .\n",
    "%cd /content/hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import nvdiffrast.torch as dr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_opengl = False # On T4 GPU, only False works, but rasterizer works much better if = True\n",
    "glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multidiffusion import MultiDiffusion\n",
    "from utils import cond_noise_sampling, erp2pers_latent_warping, compute_erp_up_noise_pred, compute_erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_5(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   -40.0), (0.0,   -30.0), (0.0,   -35.0),\n",
    "                (0.0,   -30.0), (0.0,   -25.0), (0.0,   -20.0), (0.0,   -15.0),\n",
    "                (0.0,   -10.0), (0.0,   -5.0), (0.0,   0.0), (0.0,   5.0),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   0.0), (-45.0,   0.0),\n",
    "            ]\n",
    "        # self.views = [\n",
    "        #         (0.0,   0.0),\n",
    "        #     ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = torch.zeros_like(erp_up_latent)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "                \n",
    "                count.zero_()\n",
    "                value.zero_()\n",
    "\n",
    "                for pers_latent, erp2pers_ind, view in zip(pers_latents, erp2pers_indices, self.views):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # # compute the denoising step with the reference model\n",
    "                    pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_denoised_latent for valid region\n",
    "                    erp_up_noise_denoised, erp_up_valid_region = compute_erp_up_noise_denoised(pers_latent_denoised, erp2pers_ind, fin_v_num)\n",
    "\n",
    "                    value[:, :] += torch.where(erp_up_valid_region, erp_up_noise_denoised[:, :], torch.zeros_like(erp_up_noise_denoised)[:, :])\n",
    "                    count[:, :] += erp_up_valid_region\n",
    "                    \n",
    "                    theta, phi = view\n",
    "                    erp_up_valid_region = erp_up_valid_region[0, 0]\n",
    "                    mask = ToPILImage()(erp_up_valid_region.float().cpu())\n",
    "                    mask.save(f'/{save_dir}/{i+1:0>2}/mask_{theta}_{phi}.png')\n",
    "\n",
    "                    print()\n",
    "                    print(f\"{i+1} / fin_v_num               : mean {fin_v_num.mean():>8.2f} std {fin_v_num.std():>8.2f}\")\n",
    "                    print(f\"{i+1} / pers_latent             : mean {pers_latent.abs().mean():>8.5f} std {pers_latent.abs().std():>8.5f}\")\n",
    "                    print(f\"{i+1} / pers_noise_pred         : mean {pers_noise_pred.abs().mean():>8.5f} std {pers_noise_pred.abs().std():>8.5f}\")\n",
    "                    print(f\"{i+1} / pers_latent_denoised    : mean {pers_latent_denoised.abs().mean():>8.5f} std {pers_latent_denoised.abs().std():>8.5f}\")\n",
    "                    print(f\"{i+1} / erp_up_latent           : mean {erp_up_latent[:,:,erp_up_valid_region].abs().mean():>8.5f} std {erp_up_latent[:,:,erp_up_valid_region].abs().std():>8.5f}\")\n",
    "                    print(f\"{i+1} / erp_up_noise_denoised   : mean {erp_up_noise_denoised[:,:,erp_up_valid_region].abs().mean():>8.5f} std {erp_up_noise_denoised[:,:,erp_up_valid_region].abs().std():>8.5f}\")\n",
    "\n",
    "                # average erp_up_latent on overlap region\n",
    "                count_ = torch.clamp(count, min=1) \n",
    "                erp_up_latent = value / count_\n",
    "\n",
    "                count = count.float() / count.max().float()\n",
    "                count_img = ToPILImage()(count.cpu()[0][0])\n",
    "                count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "        \n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_6(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   -40.0), (0.0,   -30.0), (0.0,   -35.0),\n",
    "                (0.0,   -30.0), (0.0,   -25.0), (0.0,   -20.0), (0.0,   -15.0),\n",
    "                (0.0,   -10.0), (0.0,   -5.0), (0.0,   0.0), (0.0,   5.0),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   0.0), (-45.0,   0.0),\n",
    "            ]\n",
    "        # self.views = [\n",
    "        #         (0.0,   0.0),\n",
    "        #     ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                prompts, \n",
    "                negative_prompts='', \n",
    "                height=512, width=1024, \n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = []  # 각 view의 feature를 저장하기 위한 리스트\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "                \n",
    "                count.zero_()\n",
    "                value.clear()  # 리스트를 초기화\n",
    "\n",
    "                for pers_latent, erp2pers_ind, view in zip(pers_latents, erp2pers_indices, self.views):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_denoised_latent for valid region\n",
    "                    erp_up_noise_denoised, erp_up_valid_region = compute_erp_up_noise_denoised(pers_latent_denoised, erp2pers_ind, fin_v_num)\n",
    "\n",
    "                    # overlap region에 대한 feature 저장\n",
    "                    value.append(torch.where(erp_up_valid_region, erp_up_noise_denoised, torch.zeros_like(erp_up_noise_denoised)))\n",
    "\n",
    "                    count[:, :] += erp_up_valid_region\n",
    "\n",
    "                    theta, phi = view\n",
    "                    erp_up_valid_region = erp_up_valid_region[0, 0]\n",
    "                    mask = ToPILImage()(erp_up_valid_region.float().cpu())\n",
    "                    mask.save(f'/{save_dir}/{i+1:0>2}/mask_{theta}_{phi}.png')\n",
    "\n",
    "                # overlap region에서 랜덤하게 feature 선택\n",
    "                value_stack = torch.stack(value, dim=0)  # [num_views, 1, C, H, W]\n",
    "\n",
    "                # 각 픽셀의 count 범위 내에서 랜덤 인덱스 생성\n",
    "                valid_counts = count[0, 0].long()  # [H, W], count는 각 픽셀의 count 수\n",
    "\n",
    "                # 랜덤 인덱스를 생성, 각 픽셀별로 0부터 (count - 1) 사이의 값을 uniform하게 샘플링\n",
    "                random_indices = torch.randint(0, valid_counts.max().item(), size=valid_counts.shape, device=self.device)\n",
    "                \n",
    "                random_indices = random_indices.float() / random_indices.max().float()\n",
    "                random_indices_img = ToPILImage()(random_indices.cpu())\n",
    "                random_indices_img.save(f'/{save_dir}/{i+1:0>2}/random_ind.png')\n",
    "\n",
    "                # 픽셀별 랜덤 인덱싱 수행\n",
    "                random_indices_expanded = random_indices[None, None, None, :, :]  # Expand to [1, 1, 1, H, W]\n",
    "                erp_up_latent = torch.gather(\n",
    "                    value_stack, 0, random_indices_expanded.expand(value_stack.shape[0], 1, value_stack.shape[2], *random_indices.shape)\n",
    "                )[0]  # Extract the [1, C, H, W] portion\n",
    "\n",
    "                count = count.float() / count.max().float()\n",
    "                count_img = ToPILImage()(count.cpu()[0][0])\n",
    "                count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "        \n",
    "        return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_6_2(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   -40.0), (0.0,   -30.0), (0.0,   -35.0),\n",
    "                (0.0,   -30.0), (0.0,   -25.0), (0.0,   -20.0), (0.0,   -15.0),\n",
    "                (0.0,   -10.0), (0.0,   -5.0), (0.0,   0.0), (0.0,   5.0),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   0.0), (-45.0,   0.0),\n",
    "            ]\n",
    "        # self.views = [\n",
    "        #         (0.0,   0.0),\n",
    "        #     ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                prompts, \n",
    "                negative_prompts='', \n",
    "                height=512, width=1024, \n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = []  # 각 view의 feature를 저장하기 위한 리스트\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "                \n",
    "                count.zero_()\n",
    "                value.clear()  # 리스트를 초기화\n",
    "\n",
    "                for pers_latent, erp2pers_ind, view in zip(pers_latents, erp2pers_indices, self.views):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_noise_pred for valid region\n",
    "                    erp_up_noise_pred, erp_up_valid_region = compute_erp_up_noise_pred(pers_noise_pred, erp2pers_ind, fin_v_num)\n",
    "                    erp_up_noise_denoised = self.scheduler.step(erp_up_noise_pred, t, erp_up_latent)['prev_sample']\n",
    "\n",
    "                    # overlap region에 대한 feature 저장\n",
    "                    value.append(torch.where(erp_up_valid_region, erp_up_noise_denoised, torch.zeros_like(erp_up_noise_denoised)))\n",
    "\n",
    "                    count[:, :] += erp_up_valid_region\n",
    "\n",
    "                    theta, phi = view\n",
    "                    erp_up_valid_region = erp_up_valid_region[0, 0]\n",
    "                    mask = ToPILImage()(erp_up_valid_region.float().cpu())\n",
    "                    mask.save(f'/{save_dir}/{i+1:0>2}/mask_{theta}_{phi}.png')\n",
    "\n",
    "                # overlap region에서 랜덤하게 feature 선택\n",
    "                value_stack = torch.stack(value, dim=0)  # [num_views, 1, C, H, W]\n",
    "\n",
    "                # 각 픽셀의 count 범위 내에서 랜덤 인덱스 생성\n",
    "                valid_counts = count[0, 0].long()  # [H, W], count는 각 픽셀의 count 수\n",
    "\n",
    "                # 랜덤 인덱스를 생성, 각 픽셀별로 0부터 (count - 1) 사이의 값을 uniform하게 샘플링\n",
    "                random_indices = torch.randint(0, valid_counts.max().item(), size=valid_counts.shape, device=self.device)\n",
    "\n",
    "                # 픽셀별 랜덤 인덱싱 수행\n",
    "                random_indices_expanded = random_indices[None, None, None, :, :]  # Expand to [1, 1, 1, H, W]\n",
    "                erp_up_latent = torch.gather(\n",
    "                    value_stack, 0, random_indices_expanded.expand(value_stack.shape[0], 1, value_stack.shape[2], *random_indices.shape)\n",
    "                )[0]  # Extract the [1, C, H, W] portion\n",
    "\n",
    "                count = count.float() / count.max().float()\n",
    "                count_img = ToPILImage()(count.cpu()[0][0])\n",
    "                count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "        \n",
    "        return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_7(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   -40.0), (0.0,   -30.0), (0.0,   -35.0),\n",
    "                (0.0,   -30.0), (0.0,   -25.0), (0.0,   -20.0), (0.0,   -15.0),\n",
    "                (0.0,   -10.0), (0.0,   -5.0), (0.0,   0.0), (0.0,   5.0),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   0.0), (-45.0,   0.0),\n",
    "            ]\n",
    "        # self.views = [\n",
    "        #         (0.0,   0.0),\n",
    "        #     ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                prompts, \n",
    "                negative_prompts='', \n",
    "                height=512, width=1024, \n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = []  # 각 view의 feature를 저장하기 위한 리스트\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "                \n",
    "                count.zero_()\n",
    "                value.clear()  # 리스트를 초기화\n",
    "\n",
    "                for pers_latent, erp2pers_ind, view in zip(pers_latents, erp2pers_indices, self.views):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_noise_pred for valid region\n",
    "                    erp_up_noise_pred, erp_up_valid_region = compute_erp_up_noise_pred(pers_noise_pred, erp2pers_ind, fin_v_num)\n",
    "                    erp_up_noise_denoised = self.scheduler.step(erp_up_noise_pred, t, erp_up_latent)['prev_sample']\n",
    "\n",
    "                    # overlap region에 대한 feature 저장\n",
    "                    value.append(torch.where(erp_up_valid_region, erp_up_noise_denoised, torch.zeros_like(erp_up_noise_denoised)))\n",
    "\n",
    "                    count[:, :] += erp_up_valid_region\n",
    "\n",
    "                    theta, phi = view\n",
    "                    erp_up_valid_region = erp_up_valid_region[0, 0]\n",
    "                    mask = ToPILImage()(erp_up_valid_region.float().cpu())\n",
    "                    mask.save(f'/{save_dir}/{i+1:0>2}/mask_{theta}_{phi}.png')\n",
    "\n",
    "\t\t\t\t# TODO: fill the below\n",
    "\n",
    "                # Initialize erp_up_latent for overlap resolution\n",
    "                erp_up_latent.zero_()\n",
    "\n",
    "                # Create random binary mask for selecting features in overlap region\n",
    "                random_mask = torch.randint(0, 2, size=value[0].shape, device=value[0].device).bool()\n",
    "\n",
    "                for idx, feature in enumerate(value):\n",
    "                    # For valid regions, add features to erp_up_latent\n",
    "                    erp_up_latent += torch.where(\n",
    "                        count == 1,  # Non-overlap region: valid region of current view only\n",
    "                        feature,\n",
    "                        torch.zeros_like(feature)\n",
    "                    )\n",
    "\n",
    "                    # For overlap region, select randomly between views\n",
    "                    if idx == 0:\n",
    "                        overlap_latent = torch.where(random_mask, feature, torch.zeros_like(feature))\n",
    "                    else:\n",
    "                        overlap_latent += torch.where(~random_mask, feature, torch.zeros_like(feature))\n",
    "\n",
    "                # Add overlap features to erp_up_latent\n",
    "                erp_up_latent += torch.where(\n",
    "                    count > 1,  # Overlap region\n",
    "                    overlap_latent,\n",
    "                    torch.zeros_like(overlap_latent)\n",
    "                )\n",
    "\n",
    "                count = count.float() / count.max().float()\n",
    "                count_img = ToPILImage()(count.cpu()[0][0])\n",
    "                count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "\n",
    "        return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_7_2(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.views = [\n",
    "                (0.0, -22.5), (15.0, -22.5), (30.0, -22.5), (-15.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0,   0.0), (15.0,   0.0), (30.0,   0.0), (-15.0,   0.0), (-30.0,   0.0),\n",
    "                (0.0,  22.5), (15.0,  22.5), (30.0,  22.5), (-15.0,  22.5), (-30.0,  22.5),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   -40.0), (0.0,   -30.0), (0.0,   -35.0),\n",
    "                (0.0,   -30.0), (0.0,   -25.0), (0.0,   -20.0), (0.0,   -15.0),\n",
    "                (0.0,   -10.0), (0.0,   -5.0), (0.0,   0.0), (0.0,   5.0),\n",
    "            ]\n",
    "        self.views = [\n",
    "                (0.0,   0.0), (-45.0,   0.0),\n",
    "            ]\n",
    "        # self.views = [\n",
    "        #         (0.0,   0.0),\n",
    "        #     ]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def latent_to_image_and_save(self, i, pers_latents, save_dir, ret_imgs):\n",
    "\n",
    "        pers_imgs = []\n",
    "        for k, pers_latent in enumerate(pers_latents):\n",
    "            pers_img = self.decode_latents(pers_latent)\n",
    "            pers_imgs.append((self.views[k], pers_img)) # [(theta, phi), img]\n",
    "        ret_imgs.append((i+1, pers_imgs)) # [i+1, [(theta, phi), img]]\n",
    "        \n",
    "        if save_dir is not None:\n",
    "            # save image\n",
    "            if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "            for v, im in pers_imgs:\n",
    "                theta, phi = v\n",
    "                im = ToPILImage()(im[0].cpu())\n",
    "                im.save(f'/{save_dir}/{i+1:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return ret_imgs\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                prompts, \n",
    "                negative_prompts='', \n",
    "                height=512, width=1024, \n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        erp_latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        # Conditional white noise sampling\n",
    "        erp_up_latent = cond_noise_sampling(erp_latent, self.up_level)\n",
    "        count = torch.zeros_like(erp_up_latent)\n",
    "        value = []  # 각 view의 feature를 저장하기 위한 리스트\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            HW_pers = (64, 64)\n",
    "\n",
    "            pers_latents, erp2pers_indices, fin_v_num =\\\n",
    "                erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "            imgs = []\n",
    "            imgs = self.latent_to_image_and_save(-1, pers_latents, save_dir, imgs)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                if os.path.exists(f\"{save_dir}/{i+1:0>2}\") is False:\n",
    "                    os.mkdir(f\"{save_dir}/{i+1:0>2}/\")\n",
    "                \n",
    "                count.zero_()\n",
    "                value.clear()  # 리스트를 초기화\n",
    "\n",
    "                for pers_latent, erp2pers_ind, view in zip(pers_latents, erp2pers_indices, self.views):\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    pers_latent_model_input = torch.cat([pers_latent] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    pers_noise_pred = self.unet(pers_latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = pers_noise_pred.chunk(2)\n",
    "                    pers_noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    pers_latent_denoised = self.scheduler.step(pers_noise_pred, t, pers_latent)['prev_sample']\n",
    "\n",
    "                    # compute erp_noise_pred for valid region\n",
    "                    erp_up_noise_denoised, erp_up_valid_region = compute_erp_up_noise_denoised(pers_latent_denoised, erp2pers_ind, fin_v_num)\n",
    "                    # erp_up_noise_denoised = self.scheduler.step(erp_up_noise_pred, t, erp_up_latent)['prev_sample']\n",
    "\n",
    "                    # overlap region에 대한 feature 저장\n",
    "                    value.append(torch.where(erp_up_valid_region, erp_up_noise_denoised, torch.zeros_like(erp_up_noise_denoised)))\n",
    "\n",
    "                    count[:, :] += erp_up_valid_region\n",
    "\n",
    "                    theta, phi = view\n",
    "                    erp_up_valid_region = erp_up_valid_region[0, 0]\n",
    "                    mask = ToPILImage()(erp_up_valid_region.float().cpu())\n",
    "                    mask.save(f'/{save_dir}/{i+1:0>2}/mask_{theta}_{phi}.png')\n",
    "\n",
    "\t\t\t\t# TODO: fill the below\n",
    "\n",
    "                # Initialize erp_up_latent for overlap resolution\n",
    "                erp_up_latent.zero_()\n",
    "\n",
    "                # Create random binary mask for selecting features in overlap region\n",
    "                random_mask = torch.randint(0, 2, size=value[0].shape, device=value[0].device).bool()\n",
    "\n",
    "                for idx, feature in enumerate(value):\n",
    "                    # For valid regions, add features to erp_up_latent\n",
    "                    erp_up_latent += torch.where(\n",
    "                        count == 1,  # Non-overlap region: valid region of current view only\n",
    "                        feature,\n",
    "                        torch.zeros_like(feature)\n",
    "                    )\n",
    "\n",
    "                    # For overlap region, select randomly between views\n",
    "                    if idx == 0:\n",
    "                        overlap_latent = torch.where(random_mask, feature, torch.zeros_like(feature))\n",
    "                    else:\n",
    "                        overlap_latent += torch.where(~random_mask, feature, torch.zeros_like(feature))\n",
    "\n",
    "                # Add overlap features to erp_up_latent\n",
    "                erp_up_latent += torch.where(\n",
    "                    count > 1,  # Overlap region\n",
    "                    overlap_latent,\n",
    "                    torch.zeros_like(overlap_latent)\n",
    "                )\n",
    "\n",
    "                count = count.float() / count.max().float()\n",
    "                count_img = ToPILImage()(count.cpu()[0][0])\n",
    "                count_img.save(f'/{save_dir}/{i+1:0>2}/count.png')\n",
    "\n",
    "                # update pers_latents from denoised erp_up_latent\n",
    "                pers_latents, _, _ = erp2pers_latent_warping(erp_up_latent, HW_pers, self.views, glctx)\n",
    "\n",
    "                imgs = self.latent_to_image_and_save(i, pers_latents, save_dir, imgs)\n",
    "\n",
    "        return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic cityscape of Florence.\"\n",
    "\n",
    "H, W = 1024, 2048\n",
    "sd = ERPMultiDiffusion_v3_5(device=device, sd_version=sd_version)\n",
    "dir_name = \"imgs\"\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}/')\n",
    "\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "outputs = sd.text2erp(prompt, negative, height=H, width=W, num_inference_steps=steps, save_dir=dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
